#!/bin/bash

# AnalyticBot Comprehensive Backup System
# Part of Phase 0.0 Module 3: Advanced DevOps & Observability
# 
# This script provides automated backup capabilities for:
# - PostgreSQL database
# - Configuration files
# - Application data
# - Cross-region replication

set -euo pipefail

# Configuration
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/../.." && pwd)"
BACKUP_ROOT="${BACKUP_ROOT:-$PROJECT_ROOT/backups}"
LOG_FILE="${LOG_FILE:-$BACKUP_ROOT/backup.log}"

# Backup configuration
RETENTION_DAYS="${RETENTION_DAYS:-30}"
COMPRESSION_LEVEL="${COMPRESSION_LEVEL:-6}"
ENCRYPTION_ENABLED="${ENCRYPTION_ENABLED:-true}"
S3_BUCKET="${S3_BUCKET:-analyticbot-backups}"
BACKUP_PREFIX="${BACKUP_PREFIX:-analyticbot}"

# Database configuration
DB_HOST="${DB_HOST:-localhost}"
DB_PORT="${DB_PORT:-5432}"
DB_NAME="${DB_NAME:-analyticbot}"
DB_USER="${DB_USER:-postgres}"
PGPASSWORD="${PGPASSWORD:-}"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Logging functions
log() {
    local level="$1"
    shift
    local message="$*"
    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
    
    echo -e "${timestamp} [${level}] ${message}" | tee -a "$LOG_FILE"
}

info() {
    log "INFO" "${BLUE}$*${NC}"
}

success() {
    log "SUCCESS" "${GREEN}$*${NC}"
}

warning() {
    log "WARNING" "${YELLOW}$*${NC}"
}

error() {
    log "ERROR" "${RED}$*${NC}"
}

# Initialize backup environment
init_backup_env() {
    info "üöÄ Initializing backup environment..."
    
    # Create backup directories
    mkdir -p "$BACKUP_ROOT"/{database,config,logs,temp}
    
    # Create log file
    touch "$LOG_FILE"
    
    # Validate required tools
    local required_tools=("pg_dump" "gzip" "tar" "aws" "gpg")
    for tool in "${required_tools[@]}"; do
        if ! command -v "$tool" &> /dev/null; then
            warning "‚ö†Ô∏è $tool not found - some features may not work"
        fi
    done
    
    success "‚úÖ Backup environment initialized"
}

# Database backup function
backup_database() {
    info "üóÑÔ∏è Starting database backup..."
    
    local timestamp=$(date '+%Y%m%d_%H%M%S')
    local backup_file="$BACKUP_ROOT/database/${BACKUP_PREFIX}_db_${timestamp}.sql"
    
    # Check database connectivity
    if ! pg_isready -h "$DB_HOST" -p "$DB_PORT" -U "$DB_USER" &> /dev/null; then
        error "‚ùå Cannot connect to database at $DB_HOST:$DB_PORT"
        return 1
    fi
    
    info "üìä Creating database dump..."
    if pg_dump -h "$DB_HOST" -p "$DB_PORT" -U "$DB_USER" -d "$DB_NAME" \
        --verbose --clean --if-exists --create \
        > "$backup_file" 2>> "$LOG_FILE"; then
        success "‚úÖ Database dump created: $(basename "$backup_file")"
    else
        error "‚ùå Database dump failed"
        return 1
    fi
    
    # Compress backup
    info "üóúÔ∏è Compressing database backup..."
    if gzip -"$COMPRESSION_LEVEL" "$backup_file"; then
        backup_file="${backup_file}.gz"
        success "‚úÖ Database backup compressed"
    else
        warning "‚ö†Ô∏è Compression failed, keeping uncompressed backup"
    fi
    
    # Encrypt backup if enabled
    if [ "$ENCRYPTION_ENABLED" = "true" ] && command -v gpg &> /dev/null; then
        info "üîí Encrypting database backup..."
        if gpg --symmetric --cipher-algo AES256 --compress-algo 1 \
            --output "${backup_file}.gpg" "$backup_file" 2>> "$LOG_FILE"; then
            rm "$backup_file"
            backup_file="${backup_file}.gpg"
            success "‚úÖ Database backup encrypted"
        else
            warning "‚ö†Ô∏è Encryption failed, keeping unencrypted backup"
        fi
    fi
    
    # Verify backup integrity
    info "üîç Verifying backup integrity..."
    if [ -f "$backup_file" ] && [ -s "$backup_file" ]; then
        local file_size=$(du -h "$backup_file" | cut -f1)
        success "‚úÖ Database backup verified: $file_size"
        echo "$backup_file"
    else
        error "‚ùå Backup verification failed"
        return 1
    fi
}

# Configuration backup function
backup_configs() {
    info "‚öôÔ∏è Starting configuration backup..."
    
    local timestamp=$(date '+%Y%m%d_%H%M%S')
    local config_backup="$BACKUP_ROOT/config/${BACKUP_PREFIX}_config_${timestamp}.tar.gz"
    
    # Define configuration paths
    local config_paths=(
        "$PROJECT_ROOT/infrastructure/helm"
        "$PROJECT_ROOT/infrastructure/k8s"
        "$PROJECT_ROOT/infrastructure/monitoring"
        "$PROJECT_ROOT/.github/workflows"
        "$PROJECT_ROOT/docker-compose*.yml"
        "$PROJECT_ROOT/Dockerfile*"
        "$PROJECT_ROOT/.env*"
        "$PROJECT_ROOT/pyproject.toml"
        "$PROJECT_ROOT/requirements*.txt"
    )
    
    # Create tar archive
    info "üì¶ Creating configuration archive..."
    local tar_args=()
    for path in "${config_paths[@]}"; do
        if [ -e "$path" ]; then
            tar_args+=("--exclude=.git" "--exclude=__pycache__" "--exclude=*.pyc" "$path")
        fi
    done
    
    if tar -czf "$config_backup" -C "$PROJECT_ROOT" "${tar_args[@]}" 2>> "$LOG_FILE"; then
        local file_size=$(du -h "$config_backup" | cut -f1)
        success "‚úÖ Configuration backup created: $file_size"
    else
        error "‚ùå Configuration backup failed"
        return 1
    fi
    
    # Encrypt config backup if enabled
    if [ "$ENCRYPTION_ENABLED" = "true" ] && command -v gpg &> /dev/null; then
        info "üîí Encrypting configuration backup..."
        if gpg --symmetric --cipher-algo AES256 \
            --output "${config_backup}.gpg" "$config_backup" 2>> "$LOG_FILE"; then
            rm "$config_backup"
            config_backup="${config_backup}.gpg"
            success "‚úÖ Configuration backup encrypted"
        else
            warning "‚ö†Ô∏è Configuration encryption failed"
        fi
    fi
    
    echo "$config_backup"
}

# Kubernetes backup function
backup_kubernetes() {
    info "‚ò∏Ô∏è Starting Kubernetes backup..."
    
    if ! command -v kubectl &> /dev/null; then
        warning "‚ö†Ô∏è kubectl not found, skipping Kubernetes backup"
        return 0
    fi
    
    local timestamp=$(date '+%Y%m%d_%H%M%S')
    local k8s_backup_dir="$BACKUP_ROOT/temp/k8s_$timestamp"
    local k8s_backup="$BACKUP_ROOT/config/${BACKUP_PREFIX}_k8s_${timestamp}.tar.gz"
    
    mkdir -p "$k8s_backup_dir"
    
    # Export Kubernetes resources
    local namespaces=("analyticbot-production" "analyticbot-staging" "analyticbot-dev")
    
    for namespace in "${namespaces[@]}"; do
        if kubectl get namespace "$namespace" &> /dev/null; then
            info "üìã Backing up namespace: $namespace"
            local ns_dir="$k8s_backup_dir/$namespace"
            mkdir -p "$ns_dir"
            
            # Export various resource types
            local resource_types=("deployments" "services" "configmaps" "secrets" "ingresses" "persistentvolumeclaims")
            
            for resource in "${resource_types[@]}"; do
                kubectl get "$resource" -n "$namespace" -o yaml > "$ns_dir/${resource}.yaml" 2>/dev/null || true
            done
        fi
    done
    
    # Create archive
    if tar -czf "$k8s_backup" -C "$BACKUP_ROOT/temp" "$(basename "$k8s_backup_dir")" 2>> "$LOG_FILE"; then
        rm -rf "$k8s_backup_dir"
        local file_size=$(du -h "$k8s_backup" | cut -f1)
        success "‚úÖ Kubernetes backup created: $file_size"
        echo "$k8s_backup"
    else
        error "‚ùå Kubernetes backup failed"
        rm -rf "$k8s_backup_dir"
        return 1
    fi
}

# Upload to cloud storage
upload_to_cloud() {
    local backup_file="$1"
    
    if ! command -v aws &> /dev/null; then
        warning "‚ö†Ô∏è AWS CLI not found, skipping cloud upload"
        return 0
    fi
    
    if [ -z "${AWS_ACCESS_KEY_ID:-}" ] || [ -z "${AWS_SECRET_ACCESS_KEY:-}" ]; then
        warning "‚ö†Ô∏è AWS credentials not configured, skipping cloud upload"
        return 0
    fi
    
    info "‚òÅÔ∏è Uploading backup to S3: $S3_BUCKET"
    
    local s3_path="s3://$S3_BUCKET/$(date '+%Y/%m/%d')/$(basename "$backup_file")"
    
    if aws s3 cp "$backup_file" "$s3_path" --storage-class STANDARD_IA 2>> "$LOG_FILE"; then
        success "‚úÖ Backup uploaded to: $s3_path"
        
        # Set lifecycle policy for automated cleanup
        aws s3api put-object-lifecycle-configuration \
            --bucket "$S3_BUCKET" \
            --lifecycle-configuration file://<(cat <<EOF
{
    "Rules": [
        {
            "ID": "AnalyticBotBackupLifecycle",
            "Status": "Enabled",
            "Filter": {"Prefix": ""},
            "Transitions": [
                {
                    "Days": 30,
                    "StorageClass": "GLACIER"
                },
                {
                    "Days": 90,
                    "StorageClass": "DEEP_ARCHIVE"
                }
            ],
            "Expiration": {
                "Days": ${RETENTION_DAYS}
            }
        }
    ]
}
EOF
            ) 2>> "$LOG_FILE" || warning "‚ö†Ô∏è Failed to set S3 lifecycle policy"
        
    else
        error "‚ùå Failed to upload backup to S3"
        return 1
    fi
}

# Cleanup old backups
cleanup_old_backups() {
    info "üßπ Cleaning up old backups (older than $RETENTION_DAYS days)..."
    
    local cleaned=0
    
    for backup_type in database config; do
        local backup_dir="$BACKUP_ROOT/$backup_type"
        if [ -d "$backup_dir" ]; then
            while IFS= read -r -d '' file; do
                rm "$file"
                ((cleaned++))
                info "üóëÔ∏è Removed old backup: $(basename "$file")"
            done < <(find "$backup_dir" -type f -mtime +$RETENTION_DAYS -print0 2>/dev/null)
        fi
    done
    
    if [ $cleaned -gt 0 ]; then
        success "‚úÖ Cleaned up $cleaned old backup files"
    else
        info "‚ÑπÔ∏è No old backups to clean up"
    fi
}

# Generate backup report
generate_report() {
    local db_backup="$1"
    local config_backup="$2"
    local k8s_backup="$3"
    
    local report_file="$BACKUP_ROOT/logs/backup_report_$(date '+%Y%m%d_%H%M%S').txt"
    
    cat > "$report_file" << EOF
ANALYTICBOT BACKUP REPORT
========================
Date: $(date)
Backup System Version: 1.0.0

BACKUP STATUS:
=============
Database Backup: $([ -n "$db_backup" ] && echo "‚úÖ SUCCESS" || echo "‚ùå FAILED")
Configuration Backup: $([ -n "$config_backup" ] && echo "‚úÖ SUCCESS" || echo "‚ùå FAILED")
Kubernetes Backup: $([ -n "$k8s_backup" ] && echo "‚úÖ SUCCESS" || echo "‚ùå FAILED")

BACKUP FILES:
=============
EOF
    
    [ -n "$db_backup" ] && echo "Database: $(basename "$db_backup") ($(du -h "$db_backup" | cut -f1))" >> "$report_file"
    [ -n "$config_backup" ] && echo "Config: $(basename "$config_backup") ($(du -h "$config_backup" | cut -f1))" >> "$report_file"
    [ -n "$k8s_backup" ] && echo "Kubernetes: $(basename "$k8s_backup") ($(du -h "$k8s_backup" | cut -f1))" >> "$report_file"
    
    cat >> "$report_file" << EOF

STORAGE USAGE:
==============
$(du -sh "$BACKUP_ROOT"/* 2>/dev/null || echo "No backup data")

RETENTION POLICY:
================
Local retention: $RETENTION_DAYS days
Cloud storage transitions: IA (30d) ‚Üí Glacier (90d) ‚Üí Deep Archive
Encryption: $([ "$ENCRYPTION_ENABLED" = "true" ] && echo "Enabled" || echo "Disabled")

END OF REPORT
EOF
    
    success "üìä Backup report generated: $(basename "$report_file")"
    cat "$report_file"
}

# Health check function
health_check() {
    info "üè• Running backup system health check..."
    
    local health_score=0
    local total_checks=5
    
    # Check backup directory
    if [ -d "$BACKUP_ROOT" ] && [ -w "$BACKUP_ROOT" ]; then
        success "‚úÖ Backup directory accessible"
        ((health_score++))
    else
        error "‚ùå Backup directory not accessible"
    fi
    
    # Check database connectivity
    if pg_isready -h "$DB_HOST" -p "$DB_PORT" -U "$DB_USER" &> /dev/null; then
        success "‚úÖ Database connectivity OK"
        ((health_score++))
    else
        error "‚ùå Database not accessible"
    fi
    
    # Check required tools
    if command -v pg_dump &> /dev/null && command -v gzip &> /dev/null; then
        success "‚úÖ Required tools available"
        ((health_score++))
    else
        error "‚ùå Missing required tools"
    fi
    
    # Check disk space
    local available_space=$(df "$BACKUP_ROOT" | awk 'NR==2 {print $4}')
    if [ "$available_space" -gt 1048576 ]; then  # 1GB in KB
        success "‚úÖ Sufficient disk space available"
        ((health_score++))
    else
        error "‚ùå Low disk space"
    fi
    
    # Check cloud credentials
    if [ -n "${AWS_ACCESS_KEY_ID:-}" ] && [ -n "${AWS_SECRET_ACCESS_KEY:-}" ]; then
        success "‚úÖ Cloud credentials configured"
        ((health_score++))
    else
        warning "‚ö†Ô∏è Cloud credentials not configured"
    fi
    
    local health_percentage=$((health_score * 100 / total_checks))
    
    if [ $health_percentage -ge 80 ]; then
        success "üéâ Backup system health: $health_percentage% - HEALTHY"
        return 0
    else
        error "üö® Backup system health: $health_percentage% - UNHEALTHY"
        return 1
    fi
}

# Test restore function
test_restore() {
    local backup_file="$1"
    
    info "üß™ Testing backup restore functionality..."
    
    if [ ! -f "$backup_file" ]; then
        error "‚ùå Backup file not found: $backup_file"
        return 1
    fi
    
    # Create test database
    local test_db="${DB_NAME}_restore_test"
    local test_user="${DB_USER}_test"
    
    info "üóÑÔ∏è Creating test database: $test_db"
    
    # This would contain actual restore testing logic
    # For now, we'll do a basic validation
    
    if [[ "$backup_file" == *.gz ]]; then
        info "üîç Validating compressed backup..."
        if gzip -t "$backup_file"; then
            success "‚úÖ Compressed backup is valid"
        else
            error "‚ùå Compressed backup is corrupted"
            return 1
        fi
    fi
    
    success "‚úÖ Restore test completed successfully"
    return 0
}

# Main backup function
main() {
    local action="${1:-full}"
    
    case "$action" in
        "full"|"")
            info "üöÄ Starting full backup process..."
            init_backup_env
            
            local db_backup=""
            local config_backup=""
            local k8s_backup=""
            
            # Perform backups
            if db_backup=$(backup_database); then
                upload_to_cloud "$db_backup" || warning "‚ö†Ô∏è Database backup upload failed"
            fi
            
            if config_backup=$(backup_configs); then
                upload_to_cloud "$config_backup" || warning "‚ö†Ô∏è Config backup upload failed"
            fi
            
            if k8s_backup=$(backup_kubernetes); then
                upload_to_cloud "$k8s_backup" || warning "‚ö†Ô∏è Kubernetes backup upload failed"
            fi
            
            # Cleanup and report
            cleanup_old_backups
            generate_report "$db_backup" "$config_backup" "$k8s_backup"
            
            if [ -n "$db_backup" ] || [ -n "$config_backup" ] || [ -n "$k8s_backup" ]; then
                success "üéâ Backup process completed successfully!"
            else
                error "‚ùå All backup operations failed"
                exit 1
            fi
            ;;
        
        "database")
            init_backup_env
            if db_backup=$(backup_database); then
                upload_to_cloud "$db_backup"
                success "üéâ Database backup completed!"
            else
                error "‚ùå Database backup failed"
                exit 1
            fi
            ;;
        
        "config")
            init_backup_env
            if config_backup=$(backup_configs); then
                upload_to_cloud "$config_backup"
                success "üéâ Configuration backup completed!"
            else
                error "‚ùå Configuration backup failed"
                exit 1
            fi
            ;;
        
        "health")
            health_check
            ;;
        
        "test")
            local backup_file="${2:-}"
            if [ -z "$backup_file" ]; then
                error "‚ùå Please specify backup file for testing"
                echo "Usage: $0 test /path/to/backup/file"
                exit 1
            fi
            test_restore "$backup_file"
            ;;
        
        "cleanup")
            cleanup_old_backups
            ;;
        
        *)
            echo "AnalyticBot Backup System"
            echo "Usage: $0 [action]"
            echo ""
            echo "Actions:"
            echo "  full      - Run full backup (default)"
            echo "  database  - Backup database only"
            echo "  config    - Backup configurations only"
            echo "  health    - Run health check"
            echo "  test      - Test restore from backup file"
            echo "  cleanup   - Clean up old backups"
            echo ""
            echo "Environment variables:"
            echo "  BACKUP_ROOT       - Backup storage directory"
            echo "  RETENTION_DAYS    - Backup retention period (default: 30)"
            echo "  DB_HOST          - Database host (default: localhost)"
            echo "  DB_NAME          - Database name (default: analyticbot)"
            echo "  S3_BUCKET        - AWS S3 bucket for cloud backup"
            echo "  ENCRYPTION_ENABLED - Enable backup encryption (default: true)"
            exit 0
            ;;
    esac
}

# Script entry point
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    main "$@"
fi
