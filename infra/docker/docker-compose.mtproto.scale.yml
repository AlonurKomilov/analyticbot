# Docker Compose configuration for MTProto scaling (Phase 4.6)
# This overlay extends the base docker-compose.yml with scaling features

version: '3.8'

services:
  # MTProto Updates Collector (Horizontally Scalable)
  mtproto-updates:
    image: ${ANALYTICBOT_IMAGE:-analyticbot:latest}
    command: python -m apps.mtproto.tasks.poll_updates
    deploy:
      replicas: 2
      restart_policy:
        condition: on-failure
        max_attempts: 3
        delay: 10s
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.2'
    environment:
      # Enable scaling features
      MTPROTO_ENABLED: "true"
      MTPROTO_UPDATES_ENABLED: "true"
      MTPROTO_POOL_ENABLED: "true"
      
      # Account pool configuration
      MTPROTO_ACCOUNTS: "session1,session2,session3"
      MTPROTO_RPS_PER_ACCOUNT: "0.7"
      MTPROTO_MAX_CONCURRENCY_PER_ACCOUNT: "2"
      MTPROTO_GLOBAL_RPS: "2.5"
      
      # Proxy pool (optional)
      MTPROTO_PROXY_ENABLED: "false"
      # MTPROTO_PROXIES: "socks5://user:pass@proxy1:1080,socks5://user:pass@proxy2:1080"
      
      # Observability
      OBS_PROMETHEUS_ENABLED: "true"
      PROMETHEUS_PORT: "9108"
      
      # Health checks
      HEALTH_BIND: "0.0.0.0:8091"
      GRACEFUL_SHUTDOWN_TIMEOUT_S: "25"
      
      # Database - Updated to match current configuration
      DATABASE_URL: "postgresql+asyncpg://analytic:change_me@postgres:5432/analytic_bot"
      REDIS_URL: "redis://redis:6379/0"
      
      # Logging
      LOG_LEVEL: "INFO"
    depends_on:
      - postgres
      - redis
    networks:
      - analyticbot-network
    volumes:
      - ./sessions:/app/sessions:rw  # Session files
      - ./logs:/app/logs:rw         # Log files
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8091/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # MTProto History Collector (Can be scaled as needed)
  mtproto-history:
    image: ${ANALYTICBOT_IMAGE:-analyticbot:latest}
    command: python -m apps.mtproto.tasks.sync_history
    deploy:
      replicas: 1  # Usually one instance is enough for history sync
      restart_policy:
        condition: on-failure
        max_attempts: 3
        delay: 30s
      resources:
        limits:
          memory: 1G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.3'
    environment:
      # Enable scaling features
      MTPROTO_ENABLED: "true"
      MTPROTO_HISTORY_ENABLED: "true"
      MTPROTO_POOL_ENABLED: "true"
      
      # Account pool configuration
      MTPROTO_ACCOUNTS: "session1,session2,session3"
      MTPROTO_RPS_PER_ACCOUNT: "0.5"  # More conservative for history
      MTPROTO_MAX_CONCURRENCY_PER_ACCOUNT: "1"
      MTPROTO_GLOBAL_RPS: "1.5"
      
      # Batch configuration
      MTPROTO_HISTORY_LIMIT_PER_RUN: "1000"
      MTPROTO_CONCURRENCY: "2"
      
      # Observability
      OBS_PROMETHEUS_ENABLED: "true"
      PROMETHEUS_PORT: "9109"  # Different port
      
      # Health checks
      HEALTH_BIND: "0.0.0.0:8092"  # Different port
      
      # Database - Updated to match current configuration
      DATABASE_URL: "postgresql+asyncpg://analytic:change_me@postgres:5432/analytic_bot"
      REDIS_URL: "redis://redis:6379/0"
    depends_on:
      - postgres
      - redis
    networks:
      - analyticbot-network
    volumes:
      - ./sessions:/app/sessions:rw
      - ./logs:/app/logs:rw
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8092/healthz"]
      interval: 60s
      timeout: 15s
      retries: 3
      start_period: 120s

  # MTProto Stats Loader (Optional - for admin channels)
  mtproto-stats:
    image: ${ANALYTICBOT_IMAGE:-analyticbot:latest}
    command: python -m apps.mtproto.tasks.load_stats
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
        max_attempts: 3
        delay: 60s
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.2'
    environment:
      # Enable stats loading
      MTPROTO_ENABLED: "true"
      MTPROTO_STATS_ENABLED: "true"
      MTPROTO_POOL_ENABLED: "true"
      
      # Stats-specific configuration
      MTPROTO_STATS_PEERS: "@your_admin_channel1,@your_admin_channel2"
      
      # Account pool
      MTPROTO_ACCOUNTS: "admin_session"  # Usually one admin account
      MTPROTO_RPS_PER_ACCOUNT: "0.3"    # Very conservative for stats API
      MTPROTO_MAX_CONCURRENCY_PER_ACCOUNT: "1"
      MTPROTO_GLOBAL_RPS: "0.5"
      
      # Observability
      OBS_PROMETHEUS_ENABLED: "true"
      PROMETHEUS_PORT: "9110"
      
      # Health checks
      HEALTH_BIND: "0.0.0.0:8093"
      
      # Database - Updated to match current configuration
      DATABASE_URL: "postgresql+asyncpg://analytic:change_me@postgres:5432/analytic_bot"
      REDIS_URL: "redis://redis:6379/0"
    depends_on:
      - postgres
      - redis
    networks:
      - analyticbot-network
    volumes:
      - ./sessions:/app/sessions:rw
      - ./logs:/app/logs:rw
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8093/healthz"]
      interval: 300s  # 5 minutes - stats loading is less frequent
      timeout: 30s
      retries: 2
      start_period: 180s

  # Prometheus for metrics collection
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9091:9090"  # Changed from 9090 to 9091 to avoid conflicts
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./monitoring/rules:/etc/prometheus/rules:ro
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=15d'
      - '--web.enable-lifecycle'
    networks:
      - analyticbot-network
    restart: unless-stopped

  # Grafana for metrics visualization
  grafana:
    image: grafana/grafana:latest
    ports:
      - "3002:3000"  # Changed from 3000 to 3002 to avoid conflicts with TWA frontend
    volumes:
      - grafana-data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources:ro
    environment:
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD:-admin}
      GF_USERS_ALLOW_SIGN_UP: "false"
      GF_PATHS_PROVISIONING: "/etc/grafana/provisioning"
    networks:
      - analyticbot-network
    restart: unless-stopped
    depends_on:
      - prometheus

  # Load balancer for health checks (optional)
  nginx:
    image: nginx:alpine
    ports:
      - "8080:80"
    volumes:
      - ./monitoring/nginx.conf:/etc/nginx/nginx.conf:ro
    networks:
      - analyticbot-network
    depends_on:
      - mtproto-updates
      - mtproto-history
      - mtproto-stats
    restart: unless-stopped

volumes:
  prometheus-data:
    driver: local
  grafana-data:
    driver: local

networks:
  analyticbot-network:
    driver: bridge

# Health check configuration notes:
# - Updates collector: Fast health checks (30s) since it should be always responsive
# - History collector: Slower health checks (60s) since batch processing can take time
# - Stats collector: Very slow health checks (5min) since stats API is rate-limited
#
# Scaling notes:
# - Updates collectors can be scaled horizontally (2+ replicas)
# - History collectors usually don't need scaling (1 replica sufficient)
# - Stats collectors should have only 1 replica (admin accounts are precious)
#
# Resource allocation:
# - Updates: Light (256M-512M RAM, 0.2-0.5 CPU)
# - History: Medium (512M-1G RAM, 0.3-1.0 CPU)
# - Stats: Light (256M-512M RAM, 0.2-0.5 CPU)
#
# To use this configuration:
# 1. Save as docker-compose.mtproto.scale.yml
# 2. Run: docker-compose -f docker-compose.yml -f docker-compose.mtproto.scale.yml up -d
# 3. Monitor via Grafana at http://localhost:3000
# 4. Check health at http://localhost:8080/health (via nginx)
#
# Environment variables to set:
# - MTPROTO_ACCOUNTS: Comma-separated session names
# - MTPROTO_STATS_PEERS: Admin channels for stats loading
# - DATABASE_URL: PostgreSQL connection string
# - REDIS_URL: Redis connection string
# - GRAFANA_PASSWORD: Grafana admin password
