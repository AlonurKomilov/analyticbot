"""
üß™ PERFORMANCE TEST RUNNER
Main entry point for running performance tests
"""

import asyncio
import logging
import sys
import time
from pathlib import Path

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent))

# Import performance testing components - create if not exists
try:
    from tests.test_performance_optimization import (
        PerformanceTester,
        run_comprehensive_performance_test,
    )
except ImportError:
    raise ImportError(
        "Performance optimization test module not found. Please ensure tests/test_performance_optimization.py exists."
    )

from bot.database.db import db_manager
from bot.database.performance import performance_manager

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler(), logging.FileHandler("performance_test.log")],
)

logger = logging.getLogger(__name__)


async def setup_test_environment():
    """üîß Setup test environment"""
    logger.info("üîß Setting up performance test environment...")

    try:
        # Initialize performance components
        await performance_manager.initialize()
        logger.info("‚úÖ Performance manager initialized")

        # Test database connection
        await db_manager.create_pool()
        health = await db_manager.health_check()

        if health:
            logger.info("‚úÖ Database connection healthy")
        else:
            logger.warning("‚ö†Ô∏è Database health check failed")

        return True

    except Exception as e:
        logger.error(f"‚ùå Environment setup failed: {e}")
        return False


async def run_basic_performance_tests():
    """üß™ Run basic performance tests"""
    logger.info("üß™ Running basic performance tests...")

    tester = PerformanceTester()

    try:
        await tester.setup()

        # Test 1: Database connection performance
        logger.info("üîç Testing database connection performance...")

        async def db_connection_test():
            async with db_manager.pool.acquire() as conn:
                result = await conn.fetchval("SELECT 1 as test_value")
                return result

        db_result = await tester.benchmark_function(
            "database_connection_basic",
            db_connection_test,
            iterations=20,
            concurrent=False,
        )

        logger.info(
            f"üìä DB Connection: {db_result.avg_time:.3f}s avg, {db_result.throughput:.1f} ops/sec"
        )

        # Test 2: Cache performance (if available)
        if performance_manager.cache._is_connected:
            logger.info("üîç Testing cache performance...")

            async def cache_test():
                key = f"test_key_{time.time()}"
                value = {"test": "data", "timestamp": time.time()}

                # Set operation
                await performance_manager.cache.set(key, value)

                # Get operation
                result = await performance_manager.cache.get(key)

                # Cleanup
                await performance_manager.cache.delete(key)

                return result is not None

            cache_result = await tester.benchmark_function(
                "cache_operations_basic", cache_test, iterations=50, concurrent=False
            )

            logger.info(
                f"üì¶ Cache: {cache_result.avg_time:.3f}s avg, {cache_result.throughput:.1f} ops/sec"
            )
        else:
            logger.warning("‚ö†Ô∏è Cache not available for testing")

        # Test 3: Concurrent operations
        logger.info("üîç Testing concurrent operations...")

        async def concurrent_test():
            tasks = []

            # Database task
            tasks.append(db_connection_test())

            # Cache task (if available)
            if performance_manager.cache._is_connected:

                async def quick_cache():
                    key = f"concurrent_test_{time.time()}"
                    await performance_manager.cache.set(key, {"concurrent": True}, 60)
                    result = await performance_manager.cache.get(key)
                    await performance_manager.cache.delete(key)
                    return result

                tasks.append(quick_cache())

            # Execute concurrently
            results = await asyncio.gather(*tasks, return_exceptions=True)
            return len([r for r in results if not isinstance(r, Exception)])

        concurrent_result = await tester.benchmark_function(
            "concurrent_operations_basic",
            concurrent_test,
            iterations=10,
            concurrent=False,
        )

        logger.info(
            f"üîÑ Concurrent: {concurrent_result.avg_time:.3f}s avg, {concurrent_result.throughput:.1f} ops/sec"
        )

        # Generate basic report
        report = tester.generate_report()

        logger.info("üìä BASIC PERFORMANCE TEST RESULTS:")
        logger.info("=" * 50)
        logger.info(f"‚úÖ Total tests: {report['summary']['total_tests']}")
        logger.info(f"‚úÖ Success rate: {report['summary']['success_rate']:.1f}%")
        logger.info(
            f"‚ö° Avg throughput: {report['summary']['avg_throughput']:.1f} ops/sec"
        )
        logger.info(
            f"‚è±Ô∏è Avg response time: {report['summary']['avg_response_time']:.3f}s"
        )

        return report

    except Exception as e:
        logger.error(f"‚ùå Basic performance tests failed: {e}")
        return None

    finally:
        await tester.teardown()


async def run_load_test():
    """üî• Run load testing"""
    logger.info("üî• Running load testing...")

    tester = PerformanceTester()

    try:
        await tester.setup()

        # Simulate high load
        async def high_load_simulation():
            """Simulate high concurrent load"""
            tasks = []

            # Create multiple concurrent database operations
            for i in range(5):  # 5 concurrent DB operations

                async def db_op():
                    async with db_manager.pool.acquire() as conn:
                        # Simulate some work
                        result = await conn.fetchval("SELECT pg_sleep(0.01), 1")
                        return result

                tasks.append(db_op())

            # Add cache operations if available
            if performance_manager.cache._is_connected:
                for i in range(10):  # 10 concurrent cache operations

                    async def cache_op():
                        key = f"load_test_{i}_{time.time()}"
                        value = {"load_test": True, "iteration": i}
                        await performance_manager.cache.set(key, value, 30)
                        result = await performance_manager.cache.get(key)
                        await performance_manager.cache.delete(key)
                        return result is not None

                    tasks.append(cache_op())

            # Execute all tasks concurrently
            results = await asyncio.gather(*tasks, return_exceptions=True)
            successful = len([r for r in results if not isinstance(r, Exception)])

            return successful

        load_result = await tester.benchmark_function(
            "high_load_simulation",
            high_load_simulation,
            iterations=5,  # Fewer iterations but more intensive
            concurrent=True,  # Run iterations concurrently too
        )

        logger.info(
            f"üî• Load Test: {load_result.avg_time:.3f}s avg, {load_result.error_rate:.1f}% errors"
        )

        # Memory usage test
        logger.info("üß† Testing memory usage...")

        import psutil

        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB

        # Create some memory load
        large_data = []
        for i in range(1000):
            large_data.append(
                {
                    "id": i,
                    "data": f"test_data_{i}" * 50,  # Larger data
                    "timestamp": time.time(),
                    "metadata": {"iteration": i, "test": True},
                }
            )

        peak_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_increase = peak_memory - initial_memory

        # Cleanup
        del large_data

        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_retained = final_memory - initial_memory

        logger.info("üß† Memory Usage:")
        logger.info(f"   üìà Peak increase: {memory_increase:.1f} MB")
        logger.info(f"   üìä Retained after cleanup: {memory_retained:.1f} MB")

        if memory_retained > 20:  # More than 20MB retained
            logger.warning(
                f"‚ö†Ô∏è Potential memory leak detected: {memory_retained:.1f} MB retained"
            )
        else:
            logger.info("‚úÖ Memory usage looks healthy")

        return {
            "load_test": load_result,
            "memory": {
                "peak_increase_mb": memory_increase,
                "retained_mb": memory_retained,
                "potential_leak": memory_retained > 20,
            },
        }

    except Exception as e:
        logger.error(f"‚ùå Load testing failed: {e}")
        return None

    finally:
        await tester.teardown()


async def run_cache_effectiveness_test():
    """üì¶ Test cache effectiveness"""
    if not performance_manager.cache._is_connected:
        logger.warning("‚ö†Ô∏è Cache not available, skipping cache effectiveness test")
        return None

    logger.info("üì¶ Testing cache effectiveness...")

    try:
        cache = performance_manager.cache

        # Test cache hit rate
        test_keys = [f"cache_test_{i}" for i in range(100)]
        test_values = [{"id": i, "data": f"test_{i}"} for i in range(100)]

        # Fill cache
        logger.info("üìù Filling cache with test data...")
        for key, value in zip(test_keys, test_values):
            await cache.set(key, value, 300)  # 5 minute TTL

        # Test hit rate
        logger.info("üîç Testing cache hit rate...")
        hits = 0
        misses = 0

        start_time = time.time()

        for key in test_keys:
            result = await cache.get(key)
            if result is not None:
                hits += 1
            else:
                misses += 1

        # Test some non-existent keys
        for i in range(20):
            result = await cache.get(f"non_existent_key_{i}")
            if result is not None:
                hits += 1
            else:
                misses += 1

        cache_time = time.time() - start_time
        total_operations = hits + misses
        hit_rate = (hits / total_operations) * 100 if total_operations > 0 else 0

        # Cleanup
        for key in test_keys:
            await cache.delete(key)

        logger.info("üìä Cache Effectiveness Results:")
        logger.info(f"   ‚úÖ Hit rate: {hit_rate:.1f}%")
        logger.info(f"   ‚ö° Operations: {total_operations}")
        logger.info(f"   ‚è±Ô∏è Total time: {cache_time:.3f}s")
        logger.info(f"   üìà Throughput: {total_operations / cache_time:.1f} ops/sec")

        return {
            "hit_rate_percent": hit_rate,
            "total_operations": total_operations,
            "total_time_seconds": cache_time,
            "throughput_ops_per_sec": total_operations / cache_time,
        }

    except Exception as e:
        logger.error(f"‚ùå Cache effectiveness test failed: {e}")
        return None


async def main():
    """üöÄ Main performance testing function"""
    logger.info("üöÄ Starting comprehensive performance testing...")

    start_time = time.time()

    # Setup test environment
    setup_success = await setup_test_environment()
    if not setup_success:
        logger.error("‚ùå Failed to setup test environment")
        return

    try:
        results = {}

        # Run basic performance tests
        logger.info("\n" + "=" * 60)
        logger.info("üß™ PHASE 1: BASIC PERFORMANCE TESTS")
        logger.info("=" * 60)

        basic_results = await run_basic_performance_tests()
        if basic_results:
            results["basic_tests"] = basic_results
            logger.info("‚úÖ Basic performance tests completed")

        # Run cache effectiveness test
        logger.info("\n" + "=" * 60)
        logger.info("üì¶ PHASE 2: CACHE EFFECTIVENESS TEST")
        logger.info("=" * 60)

        cache_results = await run_cache_effectiveness_test()
        if cache_results:
            results["cache_effectiveness"] = cache_results
            logger.info("‚úÖ Cache effectiveness test completed")

        # Run load testing
        logger.info("\n" + "=" * 60)
        logger.info("üî• PHASE 3: LOAD TESTING")
        logger.info("=" * 60)

        load_results = await run_load_test()
        if load_results:
            results["load_test"] = load_results
            logger.info("‚úÖ Load testing completed")

        # Run comprehensive test suite
        logger.info("\n" + "=" * 60)
        logger.info("üß™ PHASE 4: COMPREHENSIVE TEST SUITE")
        logger.info("=" * 60)

        comprehensive_results = await run_comprehensive_performance_test()
        if comprehensive_results:
            results["comprehensive"] = comprehensive_results
            logger.info("‚úÖ Comprehensive tests completed")

        # Generate final report
        total_time = time.time() - start_time

        logger.info("\n" + "=" * 80)
        logger.info("üèÜ FINAL PERFORMANCE TEST REPORT")
        logger.info("=" * 80)

        logger.info(f"‚è±Ô∏è Total testing time: {total_time:.2f} seconds")
        logger.info(f"üìä Test phases completed: {len(results)}/4")

        # Analyze results
        if "basic_tests" in results:
            basic = results["basic_tests"]["summary"]
            logger.info(f"üß™ Basic Tests - Success Rate: {basic['success_rate']:.1f}%")
            logger.info(
                f"   ‚ö° Average Throughput: {basic['avg_throughput']:.1f} ops/sec"
            )
            logger.info(
                f"   ‚è±Ô∏è Average Response Time: {basic['avg_response_time']:.3f}s"
            )

        if "cache_effectiveness" in results:
            cache = results["cache_effectiveness"]
            logger.info(
                f"üì¶ Cache Effectiveness: {cache['hit_rate_percent']:.1f}% hit rate"
            )
            logger.info(
                f"   ‚ö° Cache Throughput: {cache['throughput_ops_per_sec']:.1f} ops/sec"
            )

        if "load_test" in results:
            load = results["load_test"]
            if "load_test" in load:
                logger.info(
                    f"üî• Load Test - Error Rate: {load['load_test'].error_rate:.1f}%"
                )
                logger.info(f"   ‚è±Ô∏è Response Time: {load['load_test'].avg_time:.3f}s")
            if "memory" in load:
                mem = load["memory"]
                logger.info(
                    f"üß† Memory - Peak Increase: {mem['peak_increase_mb']:.1f}MB"
                )
                logger.info(
                    f"   {'‚ö†Ô∏è' if mem['potential_leak'] else '‚úÖ'} Memory Leak: {'Detected' if mem['potential_leak'] else 'None'}"
                )

        if "comprehensive" in results:
            comp = results["comprehensive"]["summary"]
            logger.info(
                f"üèÜ Comprehensive Tests - Success Rate: {comp['success_rate']:.1f}%"
            )
            logger.info(f"   üìà Total Tests: {comp['total_tests']}")

        # Performance recommendations
        logger.info("\n" + "=" * 60)
        logger.info("üí° PERFORMANCE RECOMMENDATIONS")
        logger.info("=" * 60)

        recommendations = []

        if "basic_tests" in results:
            if results["basic_tests"]["summary"]["avg_response_time"] > 0.1:
                recommendations.append("Consider optimizing database query performance")

        if "cache_effectiveness" in results:
            if results["cache_effectiveness"]["hit_rate_percent"] < 70:
                recommendations.append(
                    "Improve cache hit rate by optimizing cache strategy"
                )

        if "load_test" in results and "load_test" in results["load_test"]:
            if results["load_test"]["load_test"].error_rate > 5:
                recommendations.append("Reduce error rate under high load conditions")

        if recommendations:
            for i, rec in enumerate(recommendations, 1):
                logger.info(f"{i}. {rec}")
        else:
            logger.info("üéâ All performance metrics look excellent!")

        logger.info("\n‚úÖ Performance testing completed successfully!")

        return results

    except Exception as e:
        logger.error(f"‚ùå Performance testing failed: {e}")
        return None

    finally:
        # Cleanup
        try:
            await performance_manager.close()
            await db_manager.close_pool()
        except Exception as e:
            logger.debug(f"Cleanup error: {e}")


if __name__ == "__main__":
    # Run performance tests
    asyncio.run(main())


if __name__ == "__main__":
    # Run performance tests
    asyncio.run(main())
