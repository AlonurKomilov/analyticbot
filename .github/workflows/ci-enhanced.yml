name: Enhanced CI

on:
  pull_request:
    types: [opened, synchronize, reopened, labeled]
    branches: [main, develop, "feature/**", "release/**"]
  push:
    branches: [main, develop]
  schedule:
    # Daily CI run at 6 AM UTC
    - cron: '0 6 * * *'
  workflow_dispatch:
    inputs:
      run_performance_tests:
        description: 'Run performance benchmarks'
        required: false
        default: false
        type: boolean
      coverage_threshold:
        description: 'Coverage threshold override'
        required: false
        default: '70'
        type: string

# Global permissions
permissions:
  contents: read
  security-events: write
  pull-requests: write
  issues: write
  actions: read
  checks: write

env:
  # Environment constants
  POSTGRES_DB: analytic_bot
  POSTGRES_USER: analytic
  POSTGRES_PASSWORD: change_me_in_prod
  REDIS_URL: redis://localhost:6379/0
  TWA_HOST_URL: http://localhost:5173
  ENFORCE_PLAN_LIMITS: true

  # Performance settings
  PYTHONUNBUFFERED: 1
  PYTHONDONTWRITEBYTECODE: 1
  PIP_DISABLE_PIP_VERSION_CHECK: 1
  PIP_NO_CACHE_DIR: 1

jobs:
  # Pre-flight checks
  preflight:
    name: Pre-flight Checks
    runs-on: ubuntu-latest
    timeout-minutes: 5
    outputs:
      should_run_tests: ${{ steps.changes.outputs.should_run }}
      python_files_changed: ${{ steps.changes.outputs.python_files }}
      docker_changed: ${{ steps.changes.outputs.docker }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Check for changes
        id: changes
        run: |
          if [ "${{ github.event_name }}" = "push" ] && [ "${{ github.ref }}" = "refs/heads/main" ]; then
            echo "should_run=true" >> $GITHUB_OUTPUT
            echo "python_files=true" >> $GITHUB_OUTPUT
            echo "docker=true" >> $GITHUB_OUTPUT
          elif [ "${{ github.event_name }}" = "schedule" ] || [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            echo "should_run=true" >> $GITHUB_OUTPUT
            echo "python_files=true" >> $GITHUB_OUTPUT
            echo "docker=true" >> $GITHUB_OUTPUT
          else
            # Check changed files for PR
            CHANGED_FILES=$(git diff --name-only origin/main...HEAD)
            echo "Changed files: $CHANGED_FILES"

            if echo "$CHANGED_FILES" | grep -E '\.(py|toml|txt|yml|yaml|dockerfile)$' > /dev/null; then
              echo "should_run=true" >> $GITHUB_OUTPUT
            else
              echo "should_run=false" >> $GITHUB_OUTPUT
            fi

            if echo "$CHANGED_FILES" | grep -E '\.py$' > /dev/null; then
              echo "python_files=true" >> $GITHUB_OUTPUT
            else
              echo "python_files=false" >> $GITHUB_OUTPUT
            fi

            if echo "$CHANGED_FILES" | grep -E '(docker|Docker|compose)' > /dev/null; then
              echo "docker=true" >> $GITHUB_OUTPUT
            else
              echo "docker=false" >> $GITHUB_OUTPUT
            fi
          fi

  # Quality checks (fast)
  quality:
    name: Code Quality
    runs-on: ubuntu-latest
    needs: preflight
    if: needs.preflight.outputs.should_run_tests == 'true'
    timeout-minutes: 10

    strategy:
      matrix:
        python-version: ["3.11", "3.12"]

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Cache dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/pip
            ~/.cache/pre-commit
          key: quality-${{ runner.os }}-py${{ matrix.python-version }}-${{ hashFiles('**/pyproject.toml', '**/requirements*.txt', '.pre-commit-config.yaml') }}
          restore-keys: |
            quality-${{ runner.os }}-py${{ matrix.python-version }}-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip wheel setuptools
          pip install ruff mypy bandit safety pre-commit
          pip install -e . --no-deps

      - name: Run Ruff (lint + format check)
        run: |
          echo "::group::Ruff Check"
          ruff check . --output-format=github
          echo "::endgroup::"

          echo "::group::Ruff Format Check"
          ruff format --check .
          echo "::endgroup::"

      - name: Run MyPy (type checking)
        if: needs.preflight.outputs.python_files_changed == 'true'
        run: |
          echo "::group::MyPy Type Check"
          mypy --install-types --non-interactive .
          echo "::endgroup::"
        continue-on-error: ${{ contains(join(github.event.pull_request.labels.*.name, ','), 'mypy:softfail') }}

      - name: Security Check (Bandit)
        run: |
          echo "::group::Bandit Security Scan"
          bandit -r bot -f json -o bandit-report.json || true
          bandit -r bot --severity-level medium
          echo "::endgroup::"

      - name: Dependency Security Check
        run: |
          echo "::group::Safety Check"
          safety check --json --output safety-report.json || true
          safety check
          echo "::endgroup::"

      - name: Upload security reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: security-reports-py${{ matrix.python-version }}
          path: |
            bandit-report.json
            safety-report.json
          retention-days: 30

  # Database tests
  database-tests:
    name: Database Tests
    runs-on: ubuntu-latest
    needs: [preflight, quality]
    if: needs.preflight.outputs.should_run_tests == 'true'
    timeout-minutes: 15

    services:
      postgres:
        image: postgres:16-alpine
        env:
          POSTGRES_DB: analytic_bot
          POSTGRES_USER: analytic
          POSTGRES_PASSWORD: change_me_in_prod
        ports:
          - 5432:5432
        options: >-
          --health-cmd="pg_isready -U analytic -d analytic_bot"
          --health-interval=5s
          --health-timeout=3s
          --health-retries=30
          --shm-size=1gb

      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd="redis-cli ping"
          --health-interval=5s
          --health-timeout=3s
          --health-retries=10

    strategy:
      matrix:
        python-version: ["3.11"]  # Database tests only on one version for speed

    env:
      DATABASE_URL: postgresql+asyncpg://analytic:change_me_in_prod@localhost:5432/analytic_bot
      BOT_TOKEN: ${{ secrets.BOT_TOKEN || '1234567890:FAKE_TOKEN_FOR_TESTING' }}
      STORAGE_CHANNEL_ID: ${{ secrets.STORAGE_CHANNEL_ID || '-1001234567890' }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Cache dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: db-test-${{ runner.os }}-py${{ matrix.python-version }}-${{ hashFiles('**/pyproject.toml', '**/requirements*.txt') }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .
          pip install pytest pytest-asyncio pytest-cov pytest-xdist pytest-mock
          pip install alembic asyncpg redis

      - name: Wait for services
        run: |
          echo "Waiting for PostgreSQL..."
          for i in {1..30}; do
            pg_isready -h localhost -p 5432 -U analytic -d analytic_bot && break
            sleep 2
          done

          echo "Waiting for Redis..."
          for i in {1..15}; do
            redis-cli -h localhost -p 6379 ping && break
            sleep 2
          done

      - name: Run database migrations
        run: |
          echo "::group::Database Migration"
          alembic upgrade head
          echo "::endgroup::"

      - name: Run database tests
        run: |
          echo "::group::Database Tests"
          pytest tests/integration/ tests/unit/test_db*.py -v \
            --cov=bot --cov-report=xml --cov-report=term-missing \
            --maxfail=5 --tb=short
          echo "::endgroup::"

      - name: Upload coverage
        uses: codecov/codecov-action@v4
        with:
          files: coverage.xml
          flags: database-tests
          name: database-coverage

  # Application tests
  app-tests:
    name: Application Tests
    runs-on: ubuntu-latest
    needs: [preflight, quality]
    if: needs.preflight.outputs.should_run_tests == 'true'
    timeout-minutes: 25

    services:
      postgres:
        image: postgres:16-alpine
        env:
          POSTGRES_DB: analytic_bot
          POSTGRES_USER: analytic
          POSTGRES_PASSWORD: change_me_in_prod
        ports:
          - 5432:5432
        options: >-
          --health-cmd="pg_isready -U analytic -d analytic_bot"
          --health-interval=5s
          --health-timeout=3s
          --health-retries=30

      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd="redis-cli ping"
          --health-interval=5s
          --health-timeout=3s
          --health-retries=10

    strategy:
      fail-fast: false
      matrix:
        python-version: ["3.11", "3.12"]

    env:
      DATABASE_URL: postgresql+asyncpg://analytic:change_me_in_prod@localhost:5432/analytic_bot
      BOT_TOKEN: ${{ secrets.BOT_TOKEN || '1234567890:FAKE_TOKEN_FOR_TESTING' }}
      STORAGE_CHANNEL_ID: ${{ secrets.STORAGE_CHANNEL_ID || '-1001234567890' }}
      COV_THRESHOLD: ${{ github.event.inputs.coverage_threshold || contains(join(github.event.pull_request.labels.*.name, ','), 'cov:50') && '50' || contains(join(github.event.pull_request.labels.*.name, ','), 'cov:27') && '27' || '70' }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Cache dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: app-test-${{ runner.os }}-py${{ matrix.python-version }}-${{ hashFiles('**/pyproject.toml', '**/requirements*.txt') }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .
          pip install pytest pytest-asyncio pytest-cov pytest-xdist pytest-mock pytest-benchmark
          pip install alembic asyncpg redis httpx

      - name: Create test environment file
        run: |
          cat > .env.test <<EOF
          BOT_TOKEN=${{ env.BOT_TOKEN }}
          STORAGE_CHANNEL_ID=${{ env.STORAGE_CHANNEL_ID }}
          DATABASE_URL=${{ env.DATABASE_URL }}
          REDIS_URL=${{ env.REDIS_URL }}
          TWA_HOST_URL=${{ env.TWA_HOST_URL }}
          ENFORCE_PLAN_LIMITS=${{ env.ENFORCE_PLAN_LIMITS }}
          LOG_LEVEL=WARNING
          EOF

      - name: Wait for services
        run: |
          for i in {1..30}; do
            pg_isready -h localhost -p 5432 -U analytic -d analytic_bot && break
            sleep 2
          done

          for i in {1..15}; do
            redis-cli -h localhost -p 6379 ping && break
            sleep 2
          done

      - name: Run migrations
        run: alembic upgrade head

      - name: Run unit tests
        run: |
          echo "::group::Unit Tests"
          pytest tests/unit/ -v \
            --cov=bot --cov-report=xml --cov-report=term-missing \
            --maxfail=10 --tb=short \
            --durations=10
          echo "::endgroup::"

      - name: Run integration tests
        run: |
          echo "::group::Integration Tests"
          pytest tests/test_integration_complete.py tests/integration/ -v \
            --cov=bot --cov-append --cov-report=xml --cov-report=term-missing \
            --maxfail=5 --tb=short
          echo "::endgroup::"

      - name: Run performance benchmarks
        if: github.event.inputs.run_performance_tests == 'true' || contains(join(github.event.pull_request.labels.*.name, ','), 'performance')
        run: |
          echo "::group::Performance Benchmarks"
          pytest tests/ -v -k "benchmark" --benchmark-only --benchmark-json=benchmark.json
          echo "::endgroup::"

      - name: Check coverage threshold
        run: |
          coverage report --fail-under=${{ env.COV_THRESHOLD }}

      - name: Upload coverage
        uses: codecov/codecov-action@v4
        with:
          files: coverage.xml
          flags: app-tests-py${{ matrix.python-version }}
          name: app-coverage-py${{ matrix.python-version }}

      - name: Upload benchmark results
        if: github.event.inputs.run_performance_tests == 'true' || contains(join(github.event.pull_request.labels.*.name, ','), 'performance')
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-py${{ matrix.python-version }}
          path: benchmark.json
          retention-days: 30

  # Frontend tests
  frontend-tests:
    name: Frontend Tests
    runs-on: ubuntu-latest
    needs: preflight
    if: needs.preflight.outputs.should_run_tests == 'true'
    timeout-minutes: 15

    defaults:
      run:
        working-directory: twa-frontend

    strategy:
      matrix:
        node-version: [18, 20]

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node-version }}
          cache: 'npm'
          cache-dependency-path: twa-frontend/package-lock.json

      - name: Install dependencies
        run: npm ci

      - name: Lint check
        run: npm run lint

      - name: Type check
        run: npm run type-check
        continue-on-error: true

      - name: Run tests
        run: |
          npm run test:coverage

      - name: Build check
        run: npm run build

      - name: Bundle analysis
        run: npm run analyze || true

      - name: Upload coverage
        uses: codecov/codecov-action@v4
        with:
          files: twa-frontend/coverage/lcov.info
          flags: frontend-tests
          name: frontend-coverage

  # Docker compose tests
  compose-tests:
    name: Docker Compose Tests
    runs-on: ubuntu-latest
    needs: [preflight, quality]
    if: needs.preflight.outputs.docker_changed == 'true' || contains(join(github.event.pull_request.labels.*.name, ','), 'compose-verify:on')
    timeout-minutes: 20

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Create test environment
        run: |
          cat > .env <<EOF
          BOT_TOKEN=1234567890:FAKE_TOKEN_FOR_TESTING
          STORAGE_CHANNEL_ID=-1001234567890
          POSTGRES_USER=analytic
          POSTGRES_PASSWORD=change_me_in_prod
          POSTGRES_DB=analytic_bot
          POSTGRES_PORT=5432
          DATABASE_URL=postgresql+asyncpg://analytic:change_me_in_prod@postgres:5432/analytic_bot
          REDIS_URL=redis://redis:6379/0
          TWA_HOST_URL=http://localhost:5173
          ENFORCE_PLAN_LIMITS=true
          EOF

      - name: Build and start services
        run: |
          docker compose -f docker-compose.yml -f .github/ci.compose.override.yml up -d --build

      - name: Wait for services
        run: |
          echo "Waiting for services to be healthy..."
          timeout 300 bash -c 'until docker compose ps | grep -q "healthy"; do sleep 5; done'

      - name: Show service status
        run: docker compose ps

      - name: Test database connection
        run: |
          docker compose exec -T postgres pg_isready -U analytic -d analytic_bot

      - name: Run migrations
        run: |
          docker compose exec -T api alembic upgrade head

      - name: Test API health endpoint
        run: |
          for i in {1..30}; do
            if curl -f http://localhost:8000/health; then
              echo "API health check passed"
              break
            fi
            sleep 2
          done

      - name: Run smoke tests in container
        run: |
          docker compose exec -T api python -c "
          import asyncio
          from bot.database.db import get_pool, is_db_healthy

          async def test():
              print('Testing database connection...')
              healthy = await is_db_healthy()
              print(f'Database healthy: {healthy}')
              assert healthy, 'Database should be healthy'
              print('✅ All smoke tests passed!')

          asyncio.run(test())
          "

      - name: Collect logs
        if: always()
        run: |
          mkdir -p logs
          docker compose logs > logs/compose.log
          docker compose exec -T postgres pg_dump -U analytic analytic_bot --schema-only > logs/schema.sql || true

      - name: Upload logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: compose-logs
          path: logs/
          retention-days: 7

      - name: Cleanup
        if: always()
        run: docker compose down -v --remove-orphans

  # Summary job
  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    if: always()
    needs: [preflight, quality, database-tests, app-tests, frontend-tests, compose-tests]
    timeout-minutes: 5

    steps:
      - name: Check all test results
        run: |
          echo "## Test Results Summary" >> $GITHUB_STEP_SUMMARY
          echo "| Job | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-----|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Preflight | ${{ needs.preflight.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Quality | ${{ needs.quality.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Database Tests | ${{ needs.database-tests.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| App Tests | ${{ needs.app-tests.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Frontend Tests | ${{ needs.frontend-tests.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Compose Tests | ${{ needs.compose-tests.result }} |" >> $GITHUB_STEP_SUMMARY

      - name: Determine overall result
        run: |
          # Skip jobs are considered success
          preflight="${{ needs.preflight.result }}"
          quality="${{ needs.quality.result }}"
          db_tests="${{ needs.database-tests.result }}"
          app_tests="${{ needs.app-tests.result }}"
          frontend_tests="${{ needs.frontend-tests.result }}"
          compose_tests="${{ needs.compose-tests.result }}"

          # Check for failures (skipped is OK)
          if [[ "$preflight" == "failure" ||
                "$quality" == "failure" ||
                "$db_tests" == "failure" ||
                "$app_tests" == "failure" ||
                "$frontend_tests" == "failure" ||
                "$compose_tests" == "failure" ]]; then
            echo "❌ Some tests failed!"
            exit 1
          else
            echo "✅ All tests passed or were skipped!"
          fi
